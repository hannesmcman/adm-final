---
title: "Investigating U.S. Politicians' Twitter Habits"
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
  encoding=encoding,
  output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
author: "Johannes Carlsen, Ben Westermeyer"
date: "5/20/2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, message = FALSE, warning = FALSE, cache.lazy = FALSE)
source("scripts/helpers.R")
library(readr)
library(randomForest)
```

```{r package_options, include=FALSE}
knitr::opts_knit$set(progress = FALSE, verbose = FALSE)
```

# Introduction
Twitter enables politicians to concisely convey their policies, beliefs, and personalities to their constituents. We can learn a lot about the differences and similarities between politicians just by scrolling through their timelines. However, some patterns are still unobservable to the lay Twitter lurker. In this project, we use clustering and prediction algorithms to detect deeper patterns in the Twitter behavior of American politicians. The questions we ask address salient issues in politics, such as fake news, extreme partisanship, and unproductive lawmaking. What does Twitter have to do with all these problems? We address this in the following research questions.

# Research Questions

1. What patterns emerge when politicians' tweets are clustered based on the style of their tweets?
2. How partisan is the language of politicians' twitter accounts?
3. Is a politician's twitter activity related to his or her political activity?
4. What happens when we apply bot-detection models to politicians' tweets?

### Question #1: What patterns emerge when politicians' tweets are clustered based on the style of their tweets?

Thanks to the `rtweet` package and a dataset found at (https://github.com/unitedstates/congress-legislators), we can easily access and import all the tweets and associated twitter metadata of the currrent members of the U.S. Congress. Before diving into our classification-based research questions, it seems appropriate to search for any patterns that may exist within this twitter behaviors of these politicians. We will explore and reveal these patterns using k-means clustering and Principal Components Analysis.

With all the data provided to us by Twitter's API, we have a lot of potential variables available to us to cluster on. We will use k-means clustering and PCA on two different feature sets to help illuminate the patterns that emerge from these politicians' use of twitter. These feature sets are as follows:

1. General twitter account information (Number of tweets, number of favorites, number of followers, etc.)
2. Stylistic and lingustic features generated from the raw text of each politician's actual tweets, using the `textfeatures` package. 
  
#### Clustering on User Account Information

First we extract user account data from the Twitter API for all the U.S. legislators. Then we reduce the set of features to those that are numeric, so that they will work well with k-means clustering. Finally, we get rid of any columns with an excessive number of NA's and take out a couple outliers (Bernie Sanders and Elizabeth Warren, both of whom are running for president and have way more followers than everyone else).

```{r cluster-politicians-metadata-cleaning}
pol_accounts.df <- read.csv("data/legislators-current.csv")

pol_account_data <- lookup_users(as.character(pol_accounts.df$twitter))

pol_account_data_numeric <- select_if(pol_account_data, is.numeric)
pol_account_data_numeric$id <- pol_account_data$user_id
pol_account_data_numeric$name <- pol_account_data$name
pol_account_data_numeric$screen_name <- pol_account_data$screen_name

pol_account_data_numeric <- filter_na_columns(pol_account_data_numeric, 445)
pol_account_data_numeric <- na.omit(pol_account_data_numeric)
pol_account_data_numeric <- pol_account_data_numeric %>%
  filter(!(screen_name %in% c("SenSanders", "SenWarren")))
pol_account_data_numeric.preds <- dplyr::select(pol_account_data_numeric, -c(id, name, screen_name))
```

From the TWISS and Silhouette plots below, it looks like the optimal number of clusters is either two or three. We take a look at both. For the sake of visualization, we only show a random sample of maximum size 30 for each cluster. 

```{r cluster-politicians-metadata-clustering}
plot_twiss_and_silhouette(pol_account_data_numeric.preds, 10)
showCluster(2, pol_account_data_numeric.preds, pol_account_data_numeric$name, 30)
showCluster(3, pol_account_data_numeric.preds, pol_account_data_numeric$name, 30)
```

Visualizing the feature projections in the PCA space reveals that the separation between these clusters of politicians is based mostly on popularity, which is measured by their follower count. While this result is straightforward and perhaps not that interesting, it is still relatively amusing to be able to visualize what the "cool kids table" of Congress really looks like.

```{r cluster-politicians-metadata-pca}
mod.pca <- prcomp(pol_account_data_numeric.preds)
fviz_pca_var(mod.pca)
```

#### Clustering on Style of Tweets

A much more nuanced way to cluster our politicians based on their twitter activity is to look at the structure, or style, of their tweets themselves. We use the `textfeatures` package to derive numerical features from the raw text of these legislators' last 100 tweets, which we then use to create our clusters using k-means. We then investigate whether these clusters have anything to do with the gender or political party of these politicians.

```{r cluster-politicians-style}
load("cache/pol_nlp_data_nw2v.Rdata")
pol_nlp_data_nw2v_filtered <- pol_nlp_data_nw2v[,1:34]
pol_nlp_data_nw2v_filtered <- pol_nlp_data_nw2v_filtered %>%
  mutate(screen_name = pol_accounts.df$twitter) %>%
  mutate(party = pol_accounts.df$party) %>%
  mutate(gender = pol_accounts.df$gender) %>%
  na.omit()

pol_nlp_nw2v_cluster <- dplyr::select(pol_nlp_data_nw2v_filtered, -c(screen_name, party, gender))
```

From the TWISS and Silhouette plots below, it looks like the optimal number of clusters is either two or three. We take a look at both.

```{r cluster-politicians-style-twiss}
plot_twiss_and_silhouette(pol_nlp_nw2v_cluster, 50)
```

For the sake of understanding the plots to come, we first take a look at the feature projections onto the first two principal components. As we can see, `n_capsp` and `n_charsperword` seem to be the most influential features. 

```{r cluster-politicians-style-pca}
mod.pc <- prcomp(pol_nlp_nw2v_cluster)
fviz_pca_var(mod.pc)
```

As we can see from the plots below, the congresspeople can be separated pretty well into two clusters, but these clusters do not seem to be related to gender or party at all. Furthermore, the two clusters are of very different sizes, with almost all of the congresspeople fitting into the cluster with less variance. This seems to suggest that most congresspeople tweet very similarly, whereas a minority of legislators have their own more-unique style of tweeting. We hypothesize that the congresspeople that don't fit into the larger cluster have full control over their own accounts, rather than having a PR staff constructing their tweets for them. Unfortunately, we do not have access to the data needed to verify this hypothesis, but it seems reasonable based on the results of our clustering. 


```{r cluster-politicians-style-twiss-cluster-2}
showCluster(2, pol_nlp_nw2v_cluster, pol_nlp_data_nw2v_filtered$screen_name, 50)
showClusterWithClass(2, pol_nlp_nw2v_cluster, pol_nlp_data_nw2v_filtered$screen_name, pol_nlp_data_nw2v_filtered$party, 50)
showClusterWithClass(2, pol_nlp_nw2v_cluster, pol_nlp_data_nw2v_filtered$screen_name, pol_nlp_data_nw2v_filtered$gender, 50)
```

We can also separate the congresspeople into three clusters, which are not as distinct as the two clusters above.

```{r cluster-politicians-style-twiss-cluster-3}
showCluster(3, pol_nlp_nw2v_cluster, pol_nlp_data_nw2v_filtered$screen_name, 50)
```

### Question #2: How partisan is the language of politicians' twitter accounts?

In this section, we analyze whether Republicans and Democrats tend to communicate with their bases in fundamentally different ways. Does the sentiment, syntax, structure, or style of a Bernie Sanders tweet differ drastically from a Mitch McConnell tweet? We use the text features package to extract this kind of information from a congressperson's tweets, and use it to predict their political party.

There are three main types of text feature variables that we use as predictors:
1. Sentiment Analysis: Identifies emotion of text
2. Word2Vec: Transforms words into vectors in a vector space. Words that share common contexts are positioned in close proximity to each other in the vector space
3. Parts of Speech: Counts parts of speech used, length of words, punctuation etc.

```{r,message=FALSE, echo=FALSE}
library(rtweet)
library(httpuv)
library(readr)
library(purrr)
library(dplyr)
library(randomForest)
library(textfeatures)
library(cluster)
library(factoextra)
library(ggrepel)
library(class)
library(ranger)
library(rjson)
library(FNN)
library(gbm)
library(doBy)
library(tidyr)
library(adabag)
```

Read in congresss twitter and party data.
```{r,message=FALSE}
congress.df <- read_csv("data/legislators-current.csv")
congress.df <- congress.df[,c("govtrack_id","full_name","party","twitter")]
# Remove everyone without a twitter
congress.df <- filter(congress.df,!is.na(twitter))
# Brownley's twitter handle has changed
congress.df$twitter <- ifelse(congress.df$twitter=="JuliaBrownley26","RepBrownley",congress.df$twitter)
# Rob Bishop has never tweeted, so we should remove him
congress.df <- congress.df[congress.df$twitter!="RepRobBishop",]
# same with Roger Marshall
congress.df <- congress.df[congress.df$twitter!="RepMarshall",]
```

Use Twitter's API to get the 100 most recent tweets from each of the 529 congressmembers with active twitter acccounts.
```{r}
#timeline <- get_timelines(congress.df$twitter, n = 100,check=FALSE)

# Save the dataset created by the query so we don't have to run it every time
#saveRDS(timeline,file="Desktop/ADM R/Final Project/congress_timeline.rds")
# Now read it in
timeline <- readRDS(file="cache/congress_timeline.rds")
```

Get text features from the tweets using textfeatures package.
```{r}
#text_features <- textfeatures(timeline$text,normalize = TRUE,word_dims=50)
#saveRDS(text_features,file="Desktop/ADM R/Final Project/congress_text_features.rds")
text_features <- readRDS(file="cache/congress_text_features.rds")

# Add screen_name back into text features dataset
features.df <- cbind(twitter=timeline$screen_name,text_features)
```

Make sure we correct for any case sensitivity problems that arose during our query.
```{r}
# Check if the screen_names from timeline query match the names from congress dataset
first <- as.character(congress.df$twitter)
second <- as.character(unique(features.df$twitter))
compare <- cbind.data.frame(first,second)
mismatch <- compare[first!=second,]
# looks like 55 twitter handles don't match because of case sensitivity

# We need to replace the twitter column in congress.df with the screen_names
# from the timeline dataset
congress.df$twitter <- second
# now it should match the timeline screen names
sum(congress.df$twitter!=as.character(unique(timeline$screen_name)))
```

Now create the party variable, which will be our response variable.
```{r}
#merge in party information
congressfeatures.df <- merge(congress.df,features.df,by="twitter")

# Change party variable to binary
# Democrat and will be 0 
# Ind will also be 0 (Sen. Sanders and Sen. King both caucus with the democrats)
# Republicans will be 1
congressfeatures.df$partyind <- ifelse(congressfeatures.df$party=="Republican",1,0)
congressfeatures.df$party <- congressfeatures.df$partyind
congressfeatures.df <- select(congressfeatures.df,-partyind)

# we don't want twitter handles, IDs, or names as predictors
congressfeatures.df <- select(congressfeatures.df,-c(govtrack_id,twitter,full_name))
```

Create train and test datasets.
Note: we do not do cross validation in this section because the data is too large to do CV in a reasonable amount of time.
```{r}
party.N <- nrow(congressfeatures.df)
party.train <- sample(1:party.N,party.N/2,rep=F)
party.train.df <- congressfeatures.df[party.train,]
party.test.df <- congressfeatures.df[-party.train,]
```

### Logistic Regression
```{r}
party.logmod <- glm(party~.,family="binomial",data=party.train.df)
party.logprobs <- predict(party.logmod,newdata=party.test.df,type="response")
party.logpreds <- ifelse(party.logprobs>0.5,1,0)
    
# confusion matrix
with(party.test.df,table(party, party.logpreds))
(party.logerr <- with(party.test.df,mean(party!=party.logpreds)))
```

### Random Forest
```{r}
party.rfmod <- ranger(party~.,
                num.trees=300,
                data=party.train.df)

party.rfpredinfo <- predict(party.rfmod,data=party.test.df)
party.rfprobs <- party.rfpredinfo$predictions
party.rfpreds <- ifelse(party.rfprobs>0.5,1,0)

# confusion matrix
with(party.test.df,table(party, party.rfpreds))
(party.rferr <- mean(party.test.df$party!=party.rfpreds))
```

### Boosting
```{r}
party.gbmmod <- gbm(party ~ ., data=party.train.df,
                n.trees=300,
                distribution="bernoulli",
                #distribution="adaboost",
                interaction.depth = 2,
                shrinkage=0.1)
             

party.gbmprobs <- predict(party.gbmmod,newdata=party.test.df,n.trees=300,type="response")
party.gbmpreds <- ifelse(party.gbmprobs > 0.5,1,0)

with(party.test.df,table(party, party.gbmpreds))
(party.gbmerr <- mean(party.test.df$party!=party.gbmpreds))
```

Create a variable importance plot from the boosting model using ggplot. Show the top 20 most important variables.
```{r}
gbmImp <- varImp(party.gbmmod, scale = TRUE, numTrees=300)
impvals <- as.vector(gbmImp$Overall)
varnames <- as.vector(row.names(gbmImp))
gbminfo <- cbind(Variable=varnames,Importance=as.numeric(impvals))
gbminfo <- as.data.frame(gbminfo)
gbminfo$Variable <- as.character(gbminfo$Variable)
gbminfo$Importance <- as.character(gbminfo$Importance)
gbminfo$Importance <- as.numeric(gbminfo$Importance)
gbminfo <- gbminfo[order(gbminfo$Importance,decreasing=TRUE),]
gbminfo <- gbminfo[1:20,]

ggplot(data=gbminfo, aes(x=Variable,y=Importance)) +
  geom_bar(stat="identity") + 
  coord_flip() +
  scale_x_discrete(
    limits=gbminfo$Variable,
    labels=gbminfo$Variable 
  )+
  ggtitle("Variable Importance Plot for Predicting Party")+
  xlab("Variables")+
  ylab("Relative Importance")
```

Compare the error rate of the logistic, random foreset, and boosting algorithms.
```{r}
c(Logistic=party.logerr,RandomForest=party.rferr,Boosting=party.gbmerr)
```

All three algorithms we used have very error rates, but random forest typically performs best. It is very interesting that we are able to somewhat successfully predict the party of a congressperson just based on the structure of their tweets. This indicates that Democrats and Republicans really do have some fundamental differences in the way that they communicate with their bases.

The plot shows that the number of lower case letters in a tweet (n_lowers) is by far the strongest predictor of the tweeter's political party. It is followed by a sentiment metric (sent_vader), number of hashtags (n_hashtags), the number of polite words (n_polite), and the number of characters per word (n_charsperword). The importance of these variables indicates that the style, emotion, and complexity of a tweet can be used to determine which political party the tweeter belongs to.

### Question #3: Is a politician's twitter activity related to his or her political activity?

In this section, we explore the relationship between a politician's activity on twitter and their activity on Capitol Hill. Do politicians with a greater Twitter presence get more done in the legislature? We use congresspeople's average number of favorites, retweets, tweeting frequency, and number of followers to predict how prolific they are in congress.

Govtrack has data on the number of bills that each congressperson has introduced or cosponsored during the 116th United States Congress. It can be found here: https://www.govtrack.us/data/analysis/by-congress/116/

Read in the data on congress twitter profiles and legislative productivity.
```{r,message=FALSE}
# Sponsorship data
sponsorshipanalysis_s <- read_csv("data/sponsorshipanalysis_s.txt")
sponsorshipanalysis_h <- read_csv("data/sponsorshipanalysis_h.txt")
# Combine house and senate
sponsorship.df <- rbind(sponsorshipanalysis_s,sponsorshipanalysis_h)
```

Get one tweet from each congressperson just so we can access the date their account was created, the date of their most recent tweet, and their number of followers.
```{r}
#profiles <- get_timelines(congress.df$twitter, n = 1,check=FALSE)
#saveRDS(profiles,file="Desktop/ADM R/Final Project/congress_profiles.rds")
# Now read it in
profiles <- readRDS(file="cache/congress_profiles.rds")
```

Create tweet frequency, average tweet RT count, average tweet favorite count, and follower count variables.
```{r}
profiles$tweetfreq <- 
  as.numeric(profiles$statuses_count)/as.numeric(profiles$created_at-profiles$account_created_at)

profiles <- profiles[,c("screen_name","followers_count","tweetfreq")]

# Now let's get average RT and favorite variables from recent tweets
timeline <- as.data.frame(timeline)
rtsummary <- summaryBy(cbind(favorite_count,retweet_count)~screen_name,data=timeline,FUN=mean)

tweetinfo <- merge(x=rtsummary,y=profiles,by="screen_name")

# Get govtrack ID from congress dataset
moreinfo <- merge(x=congress.df,y=tweetinfo,by.x="twitter",by.y="screen_name")

# Merge in the productivity info using the govtrack ID
productivity.df <- merge(x=moreinfo,y=sponsorship.df,by.x="govtrack_id",by.y="ID")
productivity.df <- productivity.df[,c(5:8,14)]
#productivity.df <- data.frame(scale(productivity.df))

colnames(productivity.df) <- 
  c("Average_Favorite_Count","Average_Retweet_Count","Followers","Tweet_Frequency","Bills_Introduced")
```

Now we can predict a congressperson's number of bills introduced using specs from their twitter profiles.

### KNN

Write MSE cross validation function.
```{r}
knnMSE <- function(k,numFolds){
  
  N<-nrow(productivity.df)
  folds<-sample(1:numFolds,N,rep=T)
  
  mseKFold<-numeric(numFolds)
  
  for(fold in 1:numFolds){
    
    train.dat  <- productivity.df[folds != fold,1:4]
    train.y <- productivity.df$Bills_Introduced[folds != fold]
    test.dat   <- productivity.df[folds == fold,1:4]
    test.y <- productivity.df$Bills_Introduced[folds == fold]
    
    mod.cv  <- knn.reg(train.dat,test.dat,train.y,k=k)
    
    #test.df$pred <- predict(mod.cv,newdata=test.df)  
    mseKFold[fold] <- mean((test.y-mod.cv$pred)^2)
    
  }
  mse.kfold <- mean(mseKFold)
}

```

Try k values from 1 to 50.
```{r}
knnmses <- numeric(50)
for(i in 1:50){
  knnmses[i] <- knnMSE(i,10)
}

# report the best MSE and the k value used to get it
product.knnerr <- min(knnmses)
c(k=which.min(knnmses),MSE=product.knnerr)
```

### Random Forest

Write MSE cross validation function.
```{r}
rfMSE <- function(mtry,numTrees,numFolds){
  
  N<-nrow(productivity.df)
  folds<-sample(1:numFolds,N,rep=T)
  
  mseKFold<-numeric(numFolds)
  
  for(fold in 1:numFolds){
    
    train.df  <- productivity.df[folds != fold,]
    test.df   <- productivity.df[folds == fold,]
    
    mod.cv  <- 
      ranger(Bills_Introduced~Average_Favorite_Count+Average_Retweet_Count+Followers+Tweet_Frequency,
                 num.trees = numTrees,
                 mtry=mtry,
                 importance="impurity",
                 data=train.df)
    
    predinfo <- predict(mod.cv,data=test.df)
    preds <- predinfo$predictions
    err <- with(test.df,mean((Bills_Introduced-preds)^2))
    
    mseKFold[fold] <- err
    
  }
  mse.kfold <- mean(mseKFold)
}

```

Try a range of mtry and numtree parameter values.
```{r}
mvals <- seq(1,4)
treevals <- seq(100,300,50)
grid <- expand.grid(mvals,treevals)

rfmses <- numeric(nrow(grid))
for(i in 1:nrow(grid)){
  m <- grid[i,1]
  trees <- grid[i,2]
  mse <- rfMSE(m,trees,10)
  rfmses[i] <- mse
}

# report the best MSE and the parameters used to get it
product.rferr <- min(rfmses)
bestmtry <- grid[which.min(rfmses),1]
bestnumtrees <- grid[which.min(rfmses),2]
c(mtry=bestmtry,numtrees=bestnumtrees,MSE=product.rferr)
```

Create a model using the best parameters and look at a variable importance plot.
```{r}
train <- sample(1:nrow(productivity.df),nrow(productivity.df)/2,replace = F)
product.train.df <- productivity.df[train,]
product.test.df <- productivity.df[-train,]

product.rfmod <- ranger(Bills_Introduced~
                Average_Favorite_Count+Average_Retweet_Count+Followers+Tweet_Frequency,
                mtry <- bestmtry,
                num.trees = bestnumtrees,
                importance="impurity",
                data=product.train.df)

varimp <- as.data.frame(product.rfmod$variable.importance)
colnames(varimp) <- "Importance"
varimp$Variable <- rownames(varimp)

ggplot(varimp, aes(x=reorder(Variable,Importance), y=Importance,fill=Importance))+ 
  geom_bar(stat="identity", position="dodge")+ coord_flip()+
  ylab("Variable Importance")+
  xlab("")+
  ggtitle("Variable Importance Plot for Predicting Legislative Productivity")+
  guides(fill=F)
```

The number of followers is by far the strongest predictor of how many bills a congressperson will introduce to congress. The frequency at which a person tweets and the numer of favorites and RTs they tend to receive are not quite as important.

```{r}
c("RF Error"=product.rferr,"KNN Error"=product.knnerr)
```

How good of predictions are these? We can look at the RMSEs and compare them to the standard deviation of the response variable to get a better idea of how accurate they are.

```{r}
sqrt(c(product.knnerr,product.rferr))
sd(productivity.df$Bills_Introduced)
```

Random forest and KNN will both on average predict a value of bills within one standard deviation of the correct value. It is still not a super accurate prediction, but it can get within a reasonable range of the actual number of bills a congressperson will introduce.

### Question #4: What happens when we apply bot-detection models to politicians' tweets?

#### Part One: Building a Bot-Detection Model

In addition to our exploration of congresspeople's twitter habits, we spent a significant amount of time experimenting with building our own supervised twitter bot-detection models. Using the `textfeatures` package and several datasets found at https://botometer.iuni.iu.edu/bot-repository/datasets.html, we built a random forest bot-detection model capable of identifying real accounts versus fake accounts using a combination of account metadata and features extracted from raw tweets. The chunk below shows the process we used to create train/test datasets of a reasonable size to use with randomForest, but instead of running this chunk, we will load the train/test datasets from the cache, which is much faster. The most important part of this process was that we split the data into test/train based on user accounts first, and then populated the datasets with tweets from those users. This ensures that there is no overlap between users in our test set and our training set. 

```{r bot-detection-data-cleaning, eval=FALSE}
real_users <- read_csv("data/cresci-2017/datasets_full/genuine_accounts/users.csv")
real_users <- dplyr::select(real_users, -test_set_2)
real_user_tweets <- read_csv("data/cresci-2017/datasets_full/genuine_accounts/tweets.csv")

social_spambot_users_1 <- read_csv("data/cresci-2017/datasets_full/social_spambots_1/users.csv")
social_spambot_tweets_1 <- read_csv("data/cresci-2017/datasets_full/social_spambots_1/tweets.csv")

real_users <- real_users %>%
  mutate(real_user = TRUE)
social_spambot_users_1 <- social_spambot_users_1 %>%
  mutate(real_user = FALSE)

combined_users <- rbind(real_users, social_spambot_users_1)
test_users <- sample(1:nrow(combined_users), nrow(combined_users)/4, replace = F)
combined_tweets <- rbind(real_user_tweets, social_spambot_tweets_1)
vars_in_both <- intersect(names(combined_users), names(combined_tweets))
combined_tweets <- dplyr::select(combined_tweets, -vars_in_both)

combined_both.test <- merge(combined_users[test_users,], combined_tweets, by.x = "id", by.y = "user_id")
combined_both.train <- merge(combined_users[-test_users,], combined_tweets, by.x = "id", by.y = "user_id")
combined_both.train.filtered <- filter_na_columns(combined_both.train, 12662)
subset_train <- sample(1:nrow(combined_both.train.filtered), nrow(combined_both.train.filtered)/100, replace = F)
subset_test <- sample(1:nrow(combined_both.test), nrow(combined_both.test)/100, replace = F)
train.df <- combined_both.train.filtered[subset_train,]
test.df <- combined_both.test[subset_test,]
save(train.df, test.df, file = "cache/bot_data.Rdata")
```

Now that we have our train/test datasets ready to go, we train a random forest and logistic regression model using the predictors extracted from the Twitter API and compare their prediction error rates on the test set. 

```{r bot-detection-models}
load("cache/bot_data.Rdata")
train1.df <- dplyr::select_if(train.df, function (x) {is.numeric(x) || is.logical(x)})
train1.df <- dplyr::select(train1.df, -c(in_reply_to_status_id, retweeted_status_id, in_reply_to_user_id))
predictor_vars <- names(train1.df)
(predictor_vars <- predictor_vars[!predictor_vars == "real_user"])
## RANDOM FOREST
tweets.rf <- randomForest(factor(real_user) ~ ., data = train1.df)
tweets.rf.preds <- predict(tweets.rf, newdata = test.df)
(rf.error <- with(test.df, mean(factor(real_user) != tweets.rf.preds)))
## LOGISTIC
tweets.log <- glm(real_user ~ ., family = "binomial", data = train1.df)
tweets.log.probs <- predict(tweets.log, newdata = test.df, type = "response")
tweets.log.preds <- tweets.log.probs > 0.5
(log.error <- with(test.df, mean(real_user != tweets.log.preds)))
```

As we can see, Random Forest outperforms Logistic Regression, with a pretty low test error rate of `r rf.error`, compared to Logistic Regression's test error rate of `r log.error`. Clearly this model does a very good job at identifying bot accounts for this test dataset, likely because the test/train datasets were sourced from the same twitter conversation. It seems unlikely that this model will perform as well on a set of randomly selected tweets.

Now that we have built a bot-detection model using data given to us by the Twitter API, we investigate whether we can build an equally successful bot-detection model whose features are extracted exclusively from raw tweets, without any additional metadata. This will allow us to determine whether the language and structure of tweets from advanced bot accounts is fundamentally different from that of tweets from real users. 

```{r bot-detection-textfeatures-data, eval=FALSE}
train.textfeatures <- textfeatures(train.df$text)
train.textfeatures$real_user <- train.df$real_user
train.textfeatures <- train.textfeatures[ , colSums(is.na(train.textfeatures)) == 0]
test.textfeatures <- textfeatures(test.df$text)
test.textfeatures$real_user <- test.df$real_user
test.textfeatures <- test.textfeatures[ , colSums(is.na(test.textfeatures)) == 0]

text_features_in_both <- intersect(names(train.textfeatures), names(test.textfeatures))

train.textfeatures <- train.textfeatures[,text_features_in_both]
test.textfeatures <- train.textfeatures[,text_features_in_both]

save(train.textfeatures, test.textfeatures, file = "cache/bot-detection-textfeatures.Rdata")
```

Instead of `randomForest`, we use the more efficient `ranger` package to build our random forest model.

```{r bot-detection-textfeatures-model}
load("cache/bot-detection-textfeatures.Rdata")
textfeatures.rf <- ranger(factor(real_user) ~ ., data = train.textfeatures)

textfeatures.rf.pred <- predict(textfeatures.rf, data = test.textfeatures)
textfeatures.rf.preds <- predictions(textfeatures.rf.pred)

(textfeatures.rf.error <- with(test.textfeatures, mean(factor(real_user) != textfeatures.rf.preds)))
```

The random forest model built on features extracted from raw tweets from our training set using the `textfeatures` package has an extremely low error rate of `r textfeatures.rf.error`, which confirms that the bot accounts in our training/test sets tweet in a style and structure that is easily distinguishable from that of real users. However, as we noted before, it is unlikely that this model, which was built using a very limited subset of tweets that were all related to a similar topic, will perform as well on a random sample of tweets. Functional bot-detection models would have to be trained on a random sample of tweets in order to be less biased. However, the high level of prediction accuracy exhibited by our model is encouraging in the fight against twitter bots and fake news.

#### Part Two: How botlike are the current members of congress on twitter?

We were curious which of the current members of congress were the most bot-like, so we applied our model from the previous section to each of their last 100 tweets and ranked them in terms of the percentage of their tweets which were classified as being from bot accounts by our model. It is important to note that due to the smaller sample size of 100 tweets, we were not able to extract as many features with the `textfeatures` package, so we restricted our model to the feature space that was available to us.

```{r politicians-botlike, eval=FALSE}
load("cache/bot-detection-textfeatures.Rdata")
first_pol_account <- as.character(pol_accounts.df$twitter[1])
pol_tweets <- get_timeline(first_pol_account, n = 100)
pol.text_features <- textfeatures(pol_tweets$text)
text_features_available <- intersect(names(train.textfeatures), names(pol.text_features))
train.textfeatures_available <- train.textfeatures[,text_features_available]
train.textfeatures_available$real_user <- train.textfeatures$real_user
textfeatures_available.rf <- ranger(factor(real_user) ~ ., data = train.textfeatures_available)

names(train.textfeatures_available)

legislators_bot_like <- map_dbl(pol_accounts.df$twitter, function (screen_name) {
  how_bot_like(screen_name, 100, textfeatures_available.rf)
})
save(legislators_bot_like, file = "cache/legislators_bot_like.Rdata")
```

```{r politicians-botlike-plot}
load("cache/legislators_bot_like.Rdata")
plotData <- data.frame(name = pol_accounts.df$full_name, bot_score = legislators_bot_like)

plotData %>%
  top_n(20) %>%
  arrange(bot_score) %>%
  mutate(name = factor(name, levels = .$name)) %>%
  ggplot( aes(name, bot_score) ) +
    geom_segment( aes(x=name, xend = name, y=0, yend = bot_score), color="skyblue") +
    geom_point( color="blue", size=2, alpha=0.6) +
    theme_light() +
    coord_flip() +
    theme(
      panel.grid.major.y = element_blank(),
      panel.border = element_blank(),
      axis.ticks.y = element_blank()
    ) 
```

As you can see, our bot-prediction model performs far worse on a dataset of tweets not related to the topic from which our training set was extracted. This ranking of politician's based on their "bot-hood" likely has no bearing on anything meaningful, but it is still amusing.

# Conclusion


