---
title: "Investigating U.S. Politicians' Twitter Habits"
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
  encoding=encoding,
  output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
author: "Johannes Carlsen, Ben Westermeyer"
date: "5/20/2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, message = FALSE, warning = FALSE, cache.lazy = FALSE)
source("scripts/helpers.R")
library(readr)
library(randomForest)
```

```{r package_options, include=FALSE}
knitr::opts_knit$set(progress = FALSE, verbose = FALSE)
```

# Introduction

# Research Questions

1. What patterns emerge when politicians' tweets are clustered based on the style of their tweets?
2. How partisan is the language of politicians' twitter accounts?
3. Is a politician's twitter activity related to his or her political activity?
4. What happens when we apply bot-detection models to politicians' tweets?

### Question #1: What patterns emerge when politicians' tweets are clustered based on the style of their tweets?

Thanks to the `rtweet` package and a dataset found at (https://github.com/unitedstates/congress-legislators), we can easily access and import all the tweets and associated twitter metadata of the currrent members of the U.S. Congress. Before diving into our classification-based research questions, it seems appropriate to search for any patterns that may exist within this twitter behaviors of these politicians. We will explore and reveal these patterns using k-means clustering and Principal Components Analysis.

With all the data provided to us by Twitter's API, we have a lot of potential variables available to us to cluster on. We will use k-means clustering and PCA on two different feature sets to help illuminate the patterns that emerge from these politicians' use of twitter. These feature sets are as follows:

1. General twitter account information (Number of tweets, number of favorites, number of followers, etc.)
2. Stylistic and lingustic features generated from the raw text of each politician's actual tweets, using the `textfeatures` package. 
  
#### Clustering on User Account Information

First we extract user account data from the Twitter API for all the U.S. legislators. Then we reduce the set of features to those that are numeric, so that they will work well with k-means clustering. Finally, we get rid of any columns with an excessive number of NA's and take out a couple outliers (Bernie Sanders and Elizabeth Warren, both of whom are running for president and have way more followers than everyone else).

```{r cluster-politicians-metadata-cleaning}
pol_accounts.df <- read.csv("data/legislators-current.csv")

pol_account_data <- lookup_users(as.character(pol_accounts.df$twitter))

pol_account_data_numeric <- select_if(pol_account_data, is.numeric)
pol_account_data_numeric$id <- pol_account_data$user_id
pol_account_data_numeric$name <- pol_account_data$name
pol_account_data_numeric$screen_name <- pol_account_data$screen_name

pol_account_data_numeric <- filter_na_columns(pol_account_data_numeric, 445)
pol_account_data_numeric <- na.omit(pol_account_data_numeric)
pol_account_data_numeric <- pol_account_data_numeric %>%
  filter(!(screen_name %in% c("SenSanders", "SenWarren")))
pol_account_data_numeric.preds <- dplyr::select(pol_account_data_numeric, -c(id, name, screen_name))
```

From the TWISS and Silhouette plots below, it looks like the optimal number of clusters is either two or three. We take a look at both. For the sake of visualization, we only show a random sample of maximum size 30 for each cluster. 

```{r cluster-politicians-metadata-clustering}
plot_twiss_and_silhouette(pol_account_data_numeric.preds, 10)
showCluster(2, pol_account_data_numeric.preds, pol_account_data_numeric$name, 30)
showCluster(3, pol_account_data_numeric.preds, pol_account_data_numeric$name, 30)
```

Visualizing the feature projections in the PCA space reveals that the separation between these clusters of politicians is based mostly on popularity, which is measured by their follower count. While this result is straightforward and perhaps not that interesting, it is still relatively amusing to be able to visualize what the "cool kids table" of Congress really looks like.

```{r cluster-politicians-metadata-pca}
mod.pca <- prcomp(pol_account_data_numeric.preds)
fviz_pca_var(mod.pca)
```

#### Clustering on Style of Tweets

A much more nuanced way to cluster our politicians based on their twitter activity is to look at the structure, or style, of their tweets themselves. We use the `textfeatures` package to derive numerical features from the raw text of these legislators' last 100 tweets, which we then use to create our clusters using k-means. We then investigate whether these clusters have anything to do with the gender or political party of these politicians.

```{r cluster-politicians-style}
load("cache/pol_nlp_data_nw2v.Rdata")
pol_nlp_data_nw2v_filtered <- pol_nlp_data_nw2v[,1:34]
pol_nlp_data_nw2v_filtered <- pol_nlp_data_nw2v_filtered %>%
  mutate(screen_name = pol_accounts.df$twitter) %>%
  mutate(party = pol_accounts.df$party) %>%
  mutate(gender = pol_accounts.df$gender) %>%
  na.omit()

pol_nlp_nw2v_cluster <- dplyr::select(pol_nlp_data_nw2v_filtered, -c(screen_name, party, gender))
```

From the TWISS and Silhouette plots below, it looks like the optimal number of clusters is either two or three. We take a look at both.

```{r cluster-politicians-style-twiss}
plot_twiss_and_silhouette(pol_nlp_nw2v_cluster, 50)
```

For the sake of understanding the plots to come, we first take a look at the feature projections onto the first two principal components. As we can see, `n_capsp` and `n_charsperword` seem to be the most influential features. 

```{r cluster-politicians-style-pca}
mod.pc <- prcomp(pol_nlp_nw2v_cluster)
fviz_pca_var(mod.pc)
```

As we can see from the plots below, the congresspeople can be separated pretty well into two clusters, but these clusters do not seem to be related to gender or party at all. Furthermore, the two clusters are of very different sizes, with almost all of the congresspeople fitting into the cluster with less variance. This seems to suggest that most congresspeople tweet very similarly, whereas a minority of legislators have their own more-unique style of tweeting. We hypothesize that the congresspeople that don't fit into the larger cluster have full control over their own accounts, rather than having a PR staff constructing their tweets for them. Unfortunately, we do not have access to the data needed to verify this hypothesis, but it seems reasonable based on the results of our clustering. 


```{r cluster-politicians-style-twiss-cluster-2}
showCluster(2, pol_nlp_nw2v_cluster, pol_nlp_data_nw2v_filtered$screen_name, 50)
showClusterWithClass(2, pol_nlp_nw2v_cluster, pol_nlp_data_nw2v_filtered$screen_name, pol_nlp_data_nw2v_filtered$party, 50)
showClusterWithClass(2, pol_nlp_nw2v_cluster, pol_nlp_data_nw2v_filtered$screen_name, pol_nlp_data_nw2v_filtered$gender, 50)
```

We can also separate the congresspeople into three clusters, which are not as distinct as the two clusters above.

```{r cluster-politicians-style-twiss-cluster-3}
showCluster(3, pol_nlp_nw2v_cluster, pol_nlp_data_nw2v_filtered$screen_name, 50)
```

### Question #2: How partisan is the language of politicians' twitter accounts?

### Question #3: Is a politician's twitter activity related to his or her political activity?

### Question #4: What happens when we apply bot-detection models to politicians' tweets?

#### Part One: Building a Bot-Detection Model

In addition to our exploration of congresspeople's twitter habits, we spent a significant amount of time experimenting with building our own supervised twitter bot-detection models. Using the `textfeatures` package and several datasets found at https://botometer.iuni.iu.edu/bot-repository/datasets.html, we built a random forest bot-detection model capable of identifying real accounts versus fake accounts using a combination of account metadata and features extracted from raw tweets. The chunk below shows the process we used to create train/test datasets of a reasonable size to use with randomForest, but instead of running this chunk, we will load the train/test datasets from the cache, which is much faster. The most important part of this process was that we split the data into test/train based on user accounts first, and then populated the datasets with tweets from those users. This ensures that there is no overlap between users in our test set and our training set. 

```{r bot-detection-data-cleaning, eval=FALSE}
real_users <- read_csv("data/cresci-2017/datasets_full/genuine_accounts/users.csv")
real_users <- dplyr::select(real_users, -test_set_2)
real_user_tweets <- read_csv("data/cresci-2017/datasets_full/genuine_accounts/tweets.csv")

social_spambot_users_1 <- read_csv("data/cresci-2017/datasets_full/social_spambots_1/users.csv")
social_spambot_tweets_1 <- read_csv("data/cresci-2017/datasets_full/social_spambots_1/tweets.csv")

real_users <- real_users %>%
  mutate(real_user = TRUE)
social_spambot_users_1 <- social_spambot_users_1 %>%
  mutate(real_user = FALSE)

combined_users <- rbind(real_users, social_spambot_users_1)
test_users <- sample(1:nrow(combined_users), nrow(combined_users)/4, replace = F)
combined_tweets <- rbind(real_user_tweets, social_spambot_tweets_1)
vars_in_both <- intersect(names(combined_users), names(combined_tweets))
combined_tweets <- dplyr::select(combined_tweets, -vars_in_both)

combined_both.test <- merge(combined_users[test_users,], combined_tweets, by.x = "id", by.y = "user_id")
combined_both.train <- merge(combined_users[-test_users,], combined_tweets, by.x = "id", by.y = "user_id")
combined_both.train.filtered <- filter_na_columns(combined_both.train, 12662)
subset_train <- sample(1:nrow(combined_both.train.filtered), nrow(combined_both.train.filtered)/100, replace = F)
subset_test <- sample(1:nrow(combined_both.test), nrow(combined_both.test)/100, replace = F)
train.df <- combined_both.train.filtered[subset_train,]
test.df <- combined_both.test[subset_test,]
save(train.df, test.df, file = "cache/bot_data.Rdata")
```

Now that we have our train/test datasets ready to go, we train a random forest and logistic regression model using the predictors extracted from the Twitter API and compare their prediction error rates on the test set. 

```{r bot-detection-models}
load("cache/bot_data.Rdata")
train1.df <- dplyr::select_if(train.df, function (x) {is.numeric(x) || is.logical(x)})
train1.df <- dplyr::select(train1.df, -c(in_reply_to_status_id, retweeted_status_id, in_reply_to_user_id))
predictor_vars <- names(train1.df)
(predictor_vars <- predictor_vars[!predictor_vars == "real_user"])
## RANDOM FOREST
tweets.rf <- randomForest(factor(real_user) ~ ., data = train1.df)
tweets.rf.preds <- predict(tweets.rf, newdata = test.df)
(rf.error <- with(test.df, mean(factor(real_user) != tweets.rf.preds)))
## LOGISTIC
tweets.log <- glm(real_user ~ ., family = "binomial", data = train1.df)
tweets.log.probs <- predict(tweets.log, newdata = test.df, type = "response")
tweets.log.preds <- tweets.log.probs > 0.5
(log.error <- with(test.df, mean(real_user != tweets.log.preds)))
```

As we can see, Random Forest outperforms Logistic Regression, with a pretty low test error rate of `r rf.error`, compared to Logistic Regression's test error rate of `r log.error`. Clearly this model does a very good job at identifying bot accounts for this test dataset, likely because the test/train datasets were sourced from the same twitter conversation. It seems unlikely that this model will perform as well on a set of randomly selected tweets.

Now that we have built a bot-detection model using data given to us by the Twitter API, we investigate whether we can build an equally successful bot-detection model whose features are extracted exclusively from raw tweets, without any additional metadata. This will allow us to determine whether the language and structure of tweets from advanced bot accounts is fundamentally different from that of tweets from real users. 

```{r bot-detection-textfeatures-data, eval=FALSE}
train.textfeatures <- textfeatures(train.df$text)
train.textfeatures$real_user <- train.df$real_user
train.textfeatures <- train.textfeatures[ , colSums(is.na(train.textfeatures)) == 0]
test.textfeatures <- textfeatures(test.df$text)
test.textfeatures$real_user <- test.df$real_user
test.textfeatures <- test.textfeatures[ , colSums(is.na(test.textfeatures)) == 0]

text_features_in_both <- intersect(names(train.textfeatures), names(test.textfeatures))

train.textfeatures <- train.textfeatures[,text_features_in_both]
test.textfeatures <- train.textfeatures[,text_features_in_both]

save(train.textfeatures, test.textfeatures, file = "cache/bot-detection-textfeatures.Rdata")
```

Instead of `randomForest`, we use the more efficient `ranger` package to build our random forest model.

```{r bot-detection-textfeatures-model}
load("cache/bot-detection-textfeatures.Rdata")
textfeatures.rf <- ranger(factor(real_user) ~ ., data = train.textfeatures)

textfeatures.rf.pred <- predict(textfeatures.rf, data = test.textfeatures)
textfeatures.rf.preds <- predictions(textfeatures.rf.pred)

(textfeatures.rf.error <- with(test.textfeatures, mean(factor(real_user) != textfeatures.rf.preds)))
```

The random forest model built on features extracted from raw tweets from our training set using the `textfeatures` package has an extremely low error rate of `r textfeatures.rf.error`, which confirms that the bot accounts in our training/test sets tweet in a style and structure that is easily distinguishable from that of real users. However, as we noted before, it is unlikely that this model, which was built using a very limited subset of tweets that were all related to a similar topic, will perform as well on a random sample of tweets. Functional bot-detection models would have to be trained on a random sample of tweets in order to be less biased. However, the high level of prediction accuracy exhibited by our model is encouraging in the fight against twitter bots and fake news.

#### Part Two: How botlike are the current members of congress on twitter?

We were curious which of the current members of congress were the most bot-like, so we applied our model from the previous section to each of their last 100 tweets and ranked them in terms of the percentage of their tweets which were classified as being from bot accounts by our model. It is important to note that due to the smaller sample size of 100 tweets, we were not able to extract as many features with the `textfeatures` package, so we restricted our model to the feature space that was available to us.

```{r politicians-botlike, eval=FALSE}
load("cache/bot-detection-textfeatures.Rdata")
first_pol_account <- as.character(pol_accounts.df$twitter[1])
pol_tweets <- get_timeline(first_pol_account, n = 100)
pol.text_features <- textfeatures(pol_tweets$text)
text_features_available <- intersect(names(train.textfeatures), names(pol.text_features))
train.textfeatures_available <- train.textfeatures[,text_features_available]
train.textfeatures_available$real_user <- train.textfeatures$real_user
textfeatures_available.rf <- ranger(factor(real_user) ~ ., data = train.textfeatures_available)

names(train.textfeatures_available)

legislators_bot_like <- map_dbl(pol_accounts.df$twitter, function (screen_name) {
  how_bot_like(screen_name, 100, textfeatures_available.rf)
})
save(legislators_bot_like, file = "cache/legislators_bot_like.Rdata")
```

```{r politicians-botlike-plot}
load("cache/legislators_bot_like.Rdata")
plotData <- data.frame(name = pol_accounts.df$full_name, bot_score = legislators_bot_like)

plotData %>%
  top_n(20) %>%
  arrange(bot_score) %>%
  mutate(name = factor(name, levels = .$name)) %>%
  ggplot( aes(name, bot_score) ) +
    geom_segment( aes(x=name, xend = name, y=0, yend = bot_score), color="skyblue") +
    geom_point( color="blue", size=2, alpha=0.6) +
    theme_light() +
    coord_flip() +
    theme(
      panel.grid.major.y = element_blank(),
      panel.border = element_blank(),
      axis.ticks.y = element_blank()
    ) 
```

As you can see, our bot-prediction model performs far worse on a dataset of tweets not related to the topic from which our training set was extracted. This ranking of politician's based on their "bot-hood" likely has no bearing on anything meaningful, but it is still amusing.

# Conclusion


